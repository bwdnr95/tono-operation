#!/usr/bin/env python3
"""
ì‹¤ì œ AutoReplyService ë¡œì§ì„ ì¬í™œìš©í•œ E2E í…ŒìŠ¤íŠ¸

ì „ì²´ íŒŒì´í”„ë¼ì¸:
1. 1ì°¨ LLM (gpt-4o-mini): ì˜ë„ ë¶„ì„ â†’ pack_keys ì„ íƒ
2. Few-shot ê²€ìƒ‰: ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜
3. Answer Pack ì¡°íšŒ: ì„ íƒëœ keysì— í•´ë‹¹í•˜ëŠ” ì •ë³´
4. 2ì°¨ LLM (gpt-4.1): ìµœì¢… draft_reply ìƒì„±

ì‚¬ìš©ë²•:
    python -m app.scripts.test_e2e_reply "ìˆ˜ê±´ì´ ëª‡ ê°œ ìˆë‚˜ìš”?" -p 2S214
    python -m app.scripts.test_e2e_reply "ë°”ë² í ê°€ëŠ¥í•œê°€ìš”?" -p 2H126
    python -m app.scripts.test_e2e_reply "ì²´í¬ì¸ ì‹œê°„ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?" -p LCN
"""
import argparse
import asyncio
import json
import logging
from typing import List, Optional, Dict, Any

from dotenv import load_dotenv
load_dotenv()

from app.db.session import SessionLocal
from app.services.embedding_service import EmbeddingService
from app.services.property_answer_pack_service import PropertyAnswerPackService
from app.domain.enums.answer_pack_keys import (
    AnswerPackKey,
    ANSWER_PACK_KEY_DESCRIPTIONS,
    DEFAULT_FALLBACK_KEYS,
)
from app.domain.dtos.answer_pack_dto import AnswerPackResult
from app.adapters.llm_client import get_openai_client

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(message)s',
    datefmt='%H:%M:%S',
)
logger = logging.getLogger(__name__)

# ëª¨ë¸ ì„¤ì • (auto_reply_service.pyì™€ ë™ì¼)
MODEL_KEY_SELECTOR = "gpt-4o-mini"
MODEL_REPLY_GENERATOR = "gpt-4.1"


class E2EReplyTester:
    """E2E í…ŒìŠ¤íŠ¸ìš© í´ë˜ìŠ¤ - AutoReplyService ë¡œì§ ì¬í™œìš©"""
    
    def __init__(self):
        self.db = SessionLocal()
        self.client = get_openai_client()
        self.embedding_service = EmbeddingService(self.db)
        self.pack_service = PropertyAnswerPackService(self.db)
    
    def close(self):
        self.db.close()
    
    async def determine_required_keys(self, guest_message: str) -> List[AnswerPackKey]:
        """1ì°¨ LLM í˜¸ì¶œ: pack_keys ì„ íƒ (auto_reply_service._determine_required_keys ë™ì¼)"""
        
        key_descriptions = "\n".join([
            f"- {key.value}: {desc}"
            for key, desc in ANSWER_PACK_KEY_DESCRIPTIONS.items()
        ])
        
        system_prompt = f"""ë‹¹ì‹ ì€ ìˆ™ë°• ê²ŒìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë‹µë³€ì— í•„ìš”í•œ ì •ë³´ ìœ í˜•ì„ ì„ íƒí•˜ëŠ” AIì…ë‹ˆë‹¤.

ì•„ë˜ ëª©ë¡ì—ì„œ ê²ŒìŠ¤íŠ¸ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ í•„ìš”í•œ keyë§Œ ì„ íƒí•˜ì„¸ìš”.
ì ˆëŒ€ë¡œ ëª©ë¡ì— ì—†ëŠ” keyë¥¼ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ key:
{key_descriptions}

ê·œì¹™:
1. ê²ŒìŠ¤íŠ¸ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë° ê¼­ í•„ìš”í•œ keyë§Œ ì„ íƒ
2. ëª¨í˜¸í•˜ë©´ ê´€ë ¨ ê°€ëŠ¥ì„± ìˆëŠ” key í¬í•¨
3. ì¢…ë£Œ ì¸ì‚¬, ê°ì‚¬ ì¸ì‚¬ëŠ” key ì—†ì´ ë¹ˆ ë°°ì—´ ë°˜í™˜
4. ê²°ì œ/í™˜ë¶ˆ ê´€ë ¨ì€ ì„ íƒí•˜ì§€ ì•ŠìŒ (ë³„ë„ ì²˜ë¦¬)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{"keys": ["wifi_info", "checkin_info"]}}"""

        user_prompt = f"ê²ŒìŠ¤íŠ¸ ë©”ì‹œì§€:\n{guest_message}"

        try:
            resp = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=MODEL_KEY_SELECTOR,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=200,
            )
            
            raw_content = resp.choices[0].message.content or "{}"
            parsed = json.loads(raw_content)
            
            selected_keys = []
            for key_str in parsed.get("keys", []):
                try:
                    key = AnswerPackKey(key_str)
                    selected_keys.append(key)
                except ValueError:
                    logger.warning(f"Invalid pack key: {key_str}")
            
            return selected_keys
            
        except Exception as exc:
            logger.warning(f"KEY_SELECTION_ERROR: {exc}")
            return list(DEFAULT_FALLBACK_KEYS)
    
    def get_filtered_few_shots(
        self,
        guest_message: str,
        pack_keys: List[AnswerPackKey],
        property_code: Optional[str],
    ) -> tuple[str, list]:
        """Few-shot ê²€ìƒ‰ (auto_reply_service._get_filtered_few_shots ë™ì¼)"""
        
        try:
            similar = self.embedding_service.find_similar_answers(
                query_text=guest_message,
                property_code=property_code,
                limit=5,
                min_similarity=0.4,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‚®ê²Œ
            )
            
            if not similar:
                return "", []
            
            # pack_keys ë§¤ì¹­ í•„í„°ë§
            key_values = [k.value for k in pack_keys]
            filtered = []
            
            for ans in similar:
                if hasattr(ans, 'pack_keys') and ans.pack_keys:
                    if any(pk in key_values for pk in ans.pack_keys):
                        filtered.append(ans)
                        if len(filtered) >= 2:
                            break
                elif len(filtered) < 2:
                    filtered.append(ans)
            
            if not filtered:
                filtered = similar[:2]
            
            # í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            examples = []
            for i, ans in enumerate(filtered[:2], 1):
                examples.append(f"""### ê³¼ê±° ì‚¬ë¡€ {i} (ìœ ì‚¬ë„: {ans.similarity:.0%})
**ê²ŒìŠ¤íŠ¸ ë©”ì‹œì§€:** {ans.guest_message}
**ìŠ¹ì¸ëœ ë‹µë³€:** {ans.final_answer}""")
            
            return "\n\n".join(examples), filtered[:2]
            
        except Exception as e:
            logger.warning(f"FEW_SHOT_ERROR: {e}")
            return "", []
    
    def build_system_prompt(self) -> str:
        """System Prompt (auto_reply_service._build_system_prompt_v4 ë™ì¼)"""
        return """ROLE
ë„ˆëŠ” ìˆ™ì†Œ ìš´ì˜ìë¥¼ ëŒ€ì‹ í•´ ê²ŒìŠ¤íŠ¸ì—ê²Œ ì‹¤ì œ ì‚¬ëŒì´ ë³´ë‚¸ ê²ƒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  
ì‹ ë¢°ê° ìˆëŠ” ë‹µì¥ì„ ì‘ì„±í•œë‹¤. ëª©í‘œëŠ” ê²ŒìŠ¤íŠ¸ê°€ ì¶”ê°€ ì§ˆë¬¸ ì—†ì´, 
ì´ ë©”ì‹œì§€ í•˜ë‚˜ë¡œ ë°”ë¡œ ì´í•´í•˜ê³  í–‰ë™í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê²ƒì´ë‹¤.

ë‹µë³€ì€:
- ì§§ê³  ëª…í™•í•´ì•¼ í•˜ë©°
- ë”°ëœ»í•˜ì§€ë§Œ ê³¼ì¥ë˜ë©´ ì•ˆ ë˜ê³ 
- ê³ ê°ì„¼í„° ê³µì§€ë¬¸ì´ë‚˜ AI ê°™ì€ ë§íˆ¬ê°€ ë‚˜ë©´ ì‹¤íŒ¨ë‹¤.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
INTERNAL CONSIDERATION (ì¶œë ¥í•˜ì§€ ë§ ê²ƒ)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. PROPERTY_INFOì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ ë‹µë³€
2. PROPERTY_INFOì— ì—†ëŠ” ë‚´ìš©ì€ "í™•ì¸ í›„ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤"
3. ì•ˆì „ ì´ìŠˆ ê°ì§€ ì‹œ: ì•ˆë¶€ â†’ ê³µê° â†’ ì¡°ì¹˜/ì•ˆë‚´

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
WRITING STYLE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì •ì¤‘í•˜ê³  ë¶€ë“œëŸ¬ìš´ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•œë‹¤.

ì›ì¹™:
- ë¬¸ì¥ ëì€ "~ìŠµë‹ˆë‹¤", "~ì…ë‹ˆë‹¤", "~ì„¸ìš”", "~ì—ìš”"ë¡œ ë§ˆë¬´ë¦¬
- ë”°ëœ»í•˜ì§€ë§Œ ê²©ì‹ìˆëŠ” ëŠë‚Œ ìœ ì§€
- ì´ëª¨ì§€ëŠ” :) ğŸ˜Š ì •ë„ë§Œ ì ˆì œí•´ì„œ ì‚¬ìš© (ë¬¸ì¥ë‹¹ ìµœëŒ€ 1ê°œ)

ê¶Œì¥ íë¦„:
â‘  ì§§ì€ ì¸ì‚¬ ("ì•ˆë…•í•˜ì„¸ìš”!")
â‘¡ í•µì‹¬ ì •ë³´
â‘¢ (ì„ íƒ) ë¶€ë“œëŸ¬ìš´ ì•ˆë‚´ ("í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤")
â‘£ ì§§ì€ ë§ˆë¬´ë¦¬ ("ê°ì‚¬í•©ë‹ˆë‹¤ :)")

ê¸ˆì§€:
- ë°˜ë§, ì¤„ì„ë§, "~ìš”~" ê°™ì€ ê³¼í•œ ì¹œê·¼í•¨
- ì•µë¬´ìƒˆ ë°˜ë³µ: "~ë¼ê³  í•˜ì…¨ëŠ”ë°", "~ë¼ëŠ” ë§ì”€ ì˜ ì•Œê² ìŠµë‹ˆë‹¤"
- í˜•ì‹ì  í‘œí˜„: "ë¬¸ì˜ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤", "ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤", "í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤"
- ì¥ë¬¸ ê³µì§€ë¬¸ ìŠ¤íƒ€ì¼

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
OUTPUT FORMAT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{
  "reply_text": "ê²ŒìŠ¤íŠ¸ì—ê²Œ ë³´ë‚¼ ìµœì¢… ë‹µì¥",
  "outcome": {
    "response_outcome": "ANSWERED_GROUNDED | NEED_FOLLOW_UP | GENERAL_RESPONSE",
    "safety_outcome": "SAFE | SENSITIVE | HIGH_RISK"
  }
}"""

    def build_user_prompt(
        self,
        guest_message: str,
        answer_pack: AnswerPackResult,
        few_shots: str,
        reservation_status: str = "DURING_STAY",
    ) -> str:
        """User Prompt (auto_reply_service._build_user_prompt_v4 ë™ì¼)"""
        
        prompt_parts = [f"""â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ GUEST_MESSAGE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{guest_message.strip()}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
RESERVATION_STATUS: {reservation_status}"""]
        
        if few_shots:
            prompt_parts.append(f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š FEW_SHOT_EXAMPLES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{few_shots}""")
        
        pack_dict = answer_pack.to_prompt_dict()
        if pack_dict:
            pack_json = json.dumps(pack_dict, ensure_ascii=False, indent=2)
            prompt_parts.append(f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ PROPERTY_INFO (ì„ íƒëœ ì •ë³´ë§Œ)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{pack_json}

âš ï¸ ìœ„ ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ "í™•ì¸ í›„ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤"ë¡œ ë‹µë³€.""")
        
        prompt_parts.append("""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ JSONìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.""")
        
        return "\n".join(prompt_parts)
    
    async def generate_reply(
        self,
        guest_message: str,
        answer_pack: AnswerPackResult,
        few_shots: str,
    ) -> Dict[str, Any]:
        """2ì°¨ LLM í˜¸ì¶œ: ìµœì¢… ë‹µë³€ ìƒì„±"""
        
        system_prompt = self.build_system_prompt()
        user_prompt = self.build_user_prompt(guest_message, answer_pack, few_shots)
        
        try:
            resp = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=MODEL_REPLY_GENERATOR,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                response_format={"type": "json_object"},
                temperature=0.4,
            )
            
            raw_content = resp.choices[0].message.content or "{}"
            parsed = json.loads(raw_content)
            
            return {
                "reply_text": parsed.get("reply_text", ""),
                "outcome": parsed.get("outcome", {}),
                "user_prompt": user_prompt,  # ë””ë²„ê¹…ìš©
            }
            
        except Exception as exc:
            logger.error(f"REPLY_GENERATION_ERROR: {exc}")
            return {
                "reply_text": "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "outcome": {"response_outcome": "ERROR"},
                "user_prompt": user_prompt,
            }
    
    async def run_e2e_test(
        self,
        guest_message: str,
        property_code: Optional[str] = None,
        show_prompt: bool = False,
    ):
        """ì „ì²´ E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        print("\n" + "=" * 70)
        print("ğŸ§ª E2E í…ŒìŠ¤íŠ¸: AutoReplyService ì „ì²´ íŒŒì´í”„ë¼ì¸")
        print("=" * 70)
        print(f"\nğŸ“¨ ê²ŒìŠ¤íŠ¸ ë©”ì‹œì§€: \"{guest_message}\"")
        print(f"ğŸ  ìˆ™ì†Œ ì½”ë“œ: {property_code or '(ë¯¸ì§€ì •)'}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 1: 1ì°¨ LLM - pack_keys ì„ íƒ
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print("\n" + "-" * 70)
        print("ğŸ“‹ [STEP 1] 1ì°¨ LLM í˜¸ì¶œ (gpt-4o-mini) - ì˜ë„ ë¶„ì„")
        print("-" * 70)
        
        selected_keys = await self.determine_required_keys(guest_message)
        print(f"   âœ“ ì„ íƒëœ pack_keys: {[k.value for k in selected_keys]}")
        
        if not selected_keys:
            selected_keys = list(DEFAULT_FALLBACK_KEYS)
            print(f"   âš ï¸ Fallback keys ì‚¬ìš©: {[k.value for k in selected_keys]}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 2: Few-shot ê²€ìƒ‰
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print("\n" + "-" * 70)
        print("ğŸ“š [STEP 2] Few-shot ê²€ìƒ‰ (ì„ë² ë”© ìœ ì‚¬ë„)")
        print("-" * 70)
        
        few_shots_str, few_shots_raw = self.get_filtered_few_shots(
            guest_message, selected_keys, property_code
        )
        
        if few_shots_raw:
            for i, shot in enumerate(few_shots_raw, 1):
                print(f"\n   [{i}] ìœ ì‚¬ë„: {shot.similarity:.1%} | ìˆ™ì†Œ: {shot.property_code}")
                print(f"       Q: {shot.guest_message[:50]}...")
                print(f"       A: {shot.final_answer[:50]}...")
        else:
            print("   âŒ Few-shot ì—†ìŒ")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 3: Answer Pack ì¡°íšŒ
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print("\n" + "-" * 70)
        print("ğŸ“¦ [STEP 3] Answer Pack ì¡°íšŒ")
        print("-" * 70)
        
        if property_code:
            answer_pack = self.pack_service.get_pack(
                property_code=property_code,
                keys=selected_keys,
            )
            
            pack_dict = answer_pack.to_prompt_dict()
            if pack_dict:
                print(f"   âœ“ ì¡°íšŒëœ ì •ë³´:")
                for key, value in pack_dict.items():
                    value_str = json.dumps(value, ensure_ascii=False)
                    if len(value_str) > 80:
                        value_str = value_str[:80] + "..."
                    print(f"      - {key}: {value_str}")
            else:
                print("   âš ï¸ ë°˜í™˜ëœ ë°ì´í„° ì—†ìŒ")
        else:
            answer_pack = AnswerPackResult()
            print("   âš ï¸ property_code ì—†ìŒ - Answer Pack ì¡°íšŒ ìƒëµ")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # STEP 4: 2ì°¨ LLM - ìµœì¢… ë‹µë³€ ìƒì„±
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print("\n" + "-" * 70)
        print("ğŸ¤– [STEP 4] 2ì°¨ LLM í˜¸ì¶œ (gpt-4.1) - ë‹µë³€ ìƒì„±")
        print("-" * 70)
        
        result = await self.generate_reply(guest_message, answer_pack, few_shots_str)
        
        if show_prompt:
            print("\n   ğŸ“ User Prompt:")
            print("   " + "-" * 50)
            for line in result["user_prompt"].split("\n"):
                print(f"   {line}")
            print("   " + "-" * 50)
        
        print(f"\n   âœ“ Outcome: {result['outcome']}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ìµœì¢… ê²°ê³¼
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print("\n" + "=" * 70)
        print("ğŸ“¤ ìµœì¢… Draft Reply")
        print("=" * 70)
        print(f"\n{result['reply_text']}")
        print("\n" + "=" * 70)


async def main():
    parser = argparse.ArgumentParser(description="E2E AutoReply í…ŒìŠ¤íŠ¸")
    parser.add_argument("message", help="ê²ŒìŠ¤íŠ¸ ë©”ì‹œì§€")
    parser.add_argument("-p", "--property", help="ìˆ™ì†Œ ì½”ë“œ (ì˜ˆ: 2S214, LCN)")
    parser.add_argument("--show-prompt", action="store_true", help="LLM í”„ë¡¬í”„íŠ¸ ì¶œë ¥")
    
    args = parser.parse_args()
    
    tester = E2EReplyTester()
    try:
        await tester.run_e2e_test(
            guest_message=args.message,
            property_code=args.property,
            show_prompt=args.show_prompt,
        )
    finally:
        tester.close()


if __name__ == "__main__":
    asyncio.run(main())
